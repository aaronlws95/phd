{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import cv2 \n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "\n",
    "WORK_DIR = Path(Path.cwd()).parent\n",
    "sys.path.append(str(WORK_DIR))\n",
    "from src import ROOT\n",
    "from src.datasets.transforms import *\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'exp': 'exp1',\n",
    "    'img_root': Path(ROOT)/'EPIC_KITCHENS_2018'/'EK_frames',\n",
    "    'img_tmpl': 'img_{:05d}.jpg',\n",
    "    'img_rsz': 480,\n",
    "    'crop_sz': 80,\n",
    "    'seq_idx': 2,\n",
    "    'chosen_pt': (200, 280),\n",
    "    'len_data': 128,\n",
    "    'jitter': 20,\n",
    "    'batch_size': 32,\n",
    "    'num_workers': 8,\n",
    "    'show_pos_prob': 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_root = cfg['img_root']\n",
    "img_tmpl = cfg['img_tmpl']\n",
    "img_rsz = cfg['img_rsz']\n",
    "crop_sz = cfg['crop_sz']\n",
    "seq_idx = cfg['seq_idx']\n",
    "chosen_pt = cfg['chosen_pt']\n",
    "exp = cfg['exp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(ROOT)/'mlcv-exp'/'data'/'labels'/'ek_ar_seq_train.txt') as f:\n",
    "    img_list = f.read().splitlines()[cfg['seq_idx']]\n",
    "    \n",
    "img_path   = img_list.split(' ')[0]\n",
    "path_length = int(img_list.split(' ')[1])\n",
    "\n",
    "img_list = []\n",
    "for i in range(path_length):\n",
    "    img_list.append(img_root/img_path/img_tmpl.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose First Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img0 = cv2.imread(str(img_root/img_list[0]))[:, :, ::-1]\n",
    "img_list.append(img_root/img_path/img_tmpl.format(i))\n",
    "img0 = cv2.resize(img0, (img_rsz, img_rsz))\n",
    "\n",
    "pad = crop_sz\n",
    "x_cen = chosen_pt[1]\n",
    "y_cen = chosen_pt[0]\n",
    "bbox_w = crop_sz\n",
    "bbox_h = crop_sz\n",
    "chosen_point_bbox = np.asarray([x_cen, y_cen, bbox_w, bbox_h])\n",
    "\n",
    "chosen_point_bbox_xyxy = xywh2xyxy(chosen_point_bbox)\n",
    "x1 = np.minimum(chosen_point_bbox_xyxy[0], chosen_point_bbox_xyxy[2])\n",
    "y1 = np.minimum(chosen_point_bbox_xyxy[1], chosen_point_bbox_xyxy[3])\n",
    "x2 = np.maximum(chosen_point_bbox_xyxy[0], chosen_point_bbox_xyxy[2])\n",
    "y2 = np.maximum(chosen_point_bbox_xyxy[1], chosen_point_bbox_xyxy[3])\n",
    "\n",
    "chosen_crop = img0[y1:y2, x1:x2]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(chosen_crop)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rand_row = np.random.randint(0, img0.shape[1])\n",
    "# rand_col = np.random.randint(0, img0.shape[0])\n",
    "rand_row = chosen_pt[0] + np.random.randint(-10, 10)\n",
    "rand_col = chosen_pt[1] + np.random.randint(-10, 10)\n",
    "x_cen = rand_col\n",
    "y_cen = rand_row\n",
    "bbox_w = crop_sz\n",
    "bbox_h = crop_sz\n",
    "random_choice_bbox = np.asarray([x_cen, y_cen, bbox_w, bbox_h])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img0)\n",
    "draw_bbox(ax, chosen_point_bbox, 'b')\n",
    "draw_bbox(ax, random_choice_bbox, 'r')\n",
    "plt.show()\n",
    "\n",
    "print(bbox_iou(random_choice_bbox, chosen_point_bbox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img0_pad = np.asarray([np.pad(img0[:, :, x], pad_width=pad, mode='constant', constant_values=0) for x in range(3)])\n",
    "img0_pad = np.swapaxes(img0_pad, 0, 1)\n",
    "img0_pad = np.swapaxes(img0_pad, 1, 2)\n",
    "\n",
    "random_choice_bbox_pad = random_choice_bbox.copy()\n",
    "random_choice_bbox_pad[0] = random_choice_bbox_pad[0] + pad\n",
    "random_choice_bbox_pad[1] = random_choice_bbox_pad[1] + pad\n",
    "random_choice_bbox_pad[2] = random_choice_bbox_pad[2]\n",
    "random_choice_bbox_pad[3] = random_choice_bbox_pad[3]\n",
    "\n",
    "chosen_bbox_pad = chosen_point_bbox.copy()\n",
    "chosen_bbox_pad[0] = chosen_bbox_pad[0] + pad\n",
    "chosen_bbox_pad[1] = chosen_bbox_pad[1] + pad\n",
    "chosen_bbox_pad[2] = chosen_bbox_pad[2]\n",
    "chosen_bbox_pad[3] = chosen_bbox_pad[3]\n",
    "\n",
    "random_choice_bbox_xyxy = xywh2xyxy(random_choice_bbox)\n",
    "x1 = np.minimum(random_choice_bbox_xyxy[0], random_choice_bbox_xyxy[2]) + pad\n",
    "y1 = np.minimum(random_choice_bbox_xyxy[1], random_choice_bbox_xyxy[3]) + pad\n",
    "x2 = np.maximum(random_choice_bbox_xyxy[0], random_choice_bbox_xyxy[2]) + pad\n",
    "y2 = np.maximum(random_choice_bbox_xyxy[1], random_choice_bbox_xyxy[3]) + pad\n",
    "\n",
    "color_mask = np.zeros((img0_pad.shape[0], img0_pad.shape[1], 3))\n",
    "color_mask[y1:y2, x1:x2, 1] = 1\n",
    "rand_crop = img0_pad[y1:y2, x1:x2]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img0_pad)\n",
    "ax.imshow(color_mask, alpha=0.5)\n",
    "draw_bbox(ax, chosen_bbox_pad, 'b')\n",
    "draw_bbox(ax, random_choice_bbox_pad, 'r')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(rand_crop)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Train_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, cfg, img_path):\n",
    "        super().__init__()\n",
    "        rsz = cfg['img_rsz']\n",
    "        img_root = cfg['img_root']\n",
    "        self.crop_size = cfg['crop_sz']\n",
    "        self.chosen_point = cfg['chosen_pt']\n",
    "        self.rand = cfg['show_pos_prob']\n",
    "        self.len_data = cfg['len_data']\n",
    "        self.jitter = cfg['jitter']\n",
    "        \n",
    "        img = cv2.imread(img_path)[:, :, ::-1]\n",
    "        img = cv2.resize(img, (rsz, rsz))\n",
    "        self.img_shape = img.shape[0]\n",
    "        \n",
    "        img_pad = np.asarray([np.pad(img[:, :, x], pad_width=self.crop_size, mode='constant', constant_values=0) \n",
    "                              for x in range(3)])\n",
    "        img_pad = np.swapaxes(img_pad, 0, 1)\n",
    "        img_pad = np.swapaxes(img_pad, 1, 2)\n",
    "        self.img_pad = img_pad\n",
    "        \n",
    "        x_cen = self.chosen_point[1]\n",
    "        y_cen = self.chosen_point[0]\n",
    "        bbox_w = self.crop_size\n",
    "        bbox_h = self.crop_size\n",
    "        self.chosen_point_bbox = np.asarray([x_cen, y_cen, bbox_w, bbox_h])\n",
    "\n",
    "        tfrm = []\n",
    "        tfrm.append(ImgToTorch())\n",
    "        self.transform = torchvision.transforms.Compose(tfrm)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        np.random.seed(int(time.time()) + index)\n",
    "        rand = np.random.uniform()\n",
    "        if rand > self.rand:\n",
    "            rand_row = np.random.randint(0, self.img_shape)\n",
    "            rand_col = np.random.randint(0, self.img_shape)\n",
    "        else:\n",
    "            jitter = np.random.randint(-self.jitter, self.jitter) if self.jitter else 0\n",
    "            rand_row = self.chosen_point[0] + jitter\n",
    "            rand_col = self.chosen_point[1] + jitter\n",
    "                \n",
    "        x_cen = rand_col\n",
    "        y_cen = rand_row\n",
    "        bbox_w = self.crop_size\n",
    "        bbox_h = self.crop_size\n",
    "        random_choice_bbox = np.asarray([x_cen, y_cen, bbox_w, bbox_h])\n",
    "        random_choice_bbox_xyxy = xywh2xyxy(random_choice_bbox)\n",
    "        x1 = np.minimum(random_choice_bbox_xyxy[0], random_choice_bbox_xyxy[2]) + self.crop_size\n",
    "        y1 = np.minimum(random_choice_bbox_xyxy[1], random_choice_bbox_xyxy[3]) + self.crop_size\n",
    "        x2 = np.maximum(random_choice_bbox_xyxy[0], random_choice_bbox_xyxy[2]) + self.crop_size\n",
    "        y2 = np.maximum(random_choice_bbox_xyxy[1], random_choice_bbox_xyxy[3]) + self.crop_size\n",
    "        rand_crop = self.img_pad[y1:y2, x1:x2]\n",
    "\n",
    "        crop = rand_crop\n",
    "        iou = bbox_iou(random_choice_bbox, self.chosen_point_bbox)\n",
    "\n",
    "        sample      = {'img': crop}\n",
    "        sample      = self.transform(sample)\n",
    "        crop        = sample['img']\n",
    "        return crop, iou\n",
    "    \n",
    "    def __len__(self):\n",
    "            return self.len_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'batch_size'    : cfg['batch_size'],\n",
    "    'shuffle'       : True,\n",
    "    'num_workers'   : cfg['num_workers'],\n",
    "    'sampler'       : None,\n",
    "    'pin_memory'    : True\n",
    "}\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(Train_Dataset(cfg, str(img_root/img_list[0])), **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "resnet_feature_layers = ['conv1',\n",
    "                         'bn1',\n",
    "                         'relu',\n",
    "                         'maxpool',\n",
    "                         'layer1',\n",
    "                         'layer2',\n",
    "                         'layer3',\n",
    "                         'layer4']\n",
    "last_layer = 'layer3'\n",
    "last_layer_idx = resnet_feature_layers.index(last_layer)\n",
    "resnet_module_list = [model.conv1,\n",
    "                      model.bn1,\n",
    "                      model.relu,\n",
    "                      model.maxpool,\n",
    "                      model.layer1,\n",
    "                      model.layer2,\n",
    "                      model.layer3,\n",
    "                      model.layer4]\n",
    "resnet_model = torch.nn.Sequential(*resnet_module_list[:last_layer_idx+1])\n",
    "\n",
    "class Full_Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet_model = resnet_model\n",
    "\n",
    "        self.lin_out = torch.nn.Sequential(\n",
    "            torch.nn.Linear(25600, 512),\n",
    "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "            torch.nn.Linear(512, 256),\n",
    "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "            torch.nn.Linear(256, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.resnet_model(x)\n",
    "        out = out.view(out.shape[0], 25600)\n",
    "        out = self.lin_out(out)\n",
    "        return out\n",
    "    \n",
    "model = Full_Model()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train First Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLosses\n",
    "\n",
    "liveloss = PlotLosses()\n",
    "bce = torch.nn.BCEWithLogitsLoss()\n",
    "mse = torch.nn.MSELoss()\n",
    "max_epoch = 30\n",
    "model = model.train()\n",
    "best_loss = 1000\n",
    "for epoch in range(max_epoch):\n",
    "    logs = {}\n",
    "    running_loss = 0.0\n",
    "    for step, (img, iou) in enumerate(train_dataloader):\n",
    "#         fig, ax = plt.subplots()\n",
    "#         img = ImgToNumpy()(img)\n",
    "#         ax.imshow(img[0])\n",
    "#         plt.show()\n",
    "#         print(iou)\n",
    "        \n",
    "        img = img.cuda()\n",
    "        iou = iou.type(torch.FloatTensor).cuda()\n",
    "        \n",
    "        out = model(img)\n",
    "        \n",
    "        loss = mse(out[:, 0], iou)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    running_loss /= len(train_dataloader)\n",
    "    logs['loss'] = running_loss\n",
    "    if running_loss < best_loss:\n",
    "        best_loss = running_loss\n",
    "        state = {'epoch': epoch, \n",
    "                 'model_state_dict': model.state_dict(), \n",
    "                 'optimizer_state_dict': optimizer.state_dict()}\n",
    "        torch.save(state, Path(ROOT)/'mlcv-exp'/'data'/'weights'/'model_apt_{}.state'.format(exp))\n",
    "    liveloss.update(logs)\n",
    "    liveloss.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(best_loss)\n",
    "map_loc = 'cuda:0' \n",
    "load_dir = Path(ROOT)/'mlcv-exp'/'data'/'weights'/'model_apt_{}.state'.format(exp)\n",
    "ckpt = torch.load(load_dir, map_location=map_loc)\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "optimizer.load_state_dict(ckpt['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrm = []\n",
    "tfrm.append(ImgToTorch())\n",
    "transform = torchvision.transforms.Compose(tfrm)\n",
    "steps = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img0 = cv2.imread(str(img_root/img_list[1]))[:, :, ::-1]\n",
    "img0 = cv2.resize(img0, (img_rsz, img_rsz))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img0)\n",
    "plt.show()\n",
    "\n",
    "img_crops = []\n",
    "pos_list = []\n",
    "for row in range(0, img0.shape[1], steps):\n",
    "    for col in range(0, img0.shape[0], steps):\n",
    "        if img0[row:row+crop_sz, col:col+crop_sz].shape == (80, 80, 3):\n",
    "            img_crops.append(img0[row:row+crop_sz, col:col+crop_sz])\n",
    "            pos_list.append((row, col))\n",
    "\n",
    "crop_rows = img0.shape[1]//steps        \n",
    "crop_cols = img0.shape[0]//steps\n",
    "fig, ax = plt.subplots(crop_rows, crop_cols)\n",
    "idx = 0\n",
    "\n",
    "for i in range(crop_rows):\n",
    "    for j in range(crop_cols):\n",
    "        if idx >= len(img_crops):\n",
    "            break\n",
    "        ax[i, j].imshow(img_crops[idx])\n",
    "        ax[i, j].axis('off')\n",
    "        idx += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "pred_iou = []\n",
    "with torch.no_grad():\n",
    "    for crop in img_crops:\n",
    "        img = crop.copy()\n",
    "        sample      = {'img': img}\n",
    "        sample      = transform(sample)\n",
    "        img         = sample['img']\n",
    "        img         = img.unsqueeze(0)\n",
    "        img = img.cuda()\n",
    "        out = model(img)\n",
    "        out = out[0]\n",
    "        pred_iou.append(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(pred_iou))\n",
    "\n",
    "pos = pos_list[np.argmax(pred_iou)]\n",
    "\n",
    "color_mask = np.zeros((img0.shape[0], img0.shape[1], 3))\n",
    "color_mask[pos[0]:pos[0]+crop_sz, pos[1]:pos[1]+crop_sz, 1] = 1\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img0)\n",
    "ax.imshow(color_mask, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import ImageSequenceClip\n",
    "from tqdm import tqdm\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "from IPython.display import Image as IPythonImage\n",
    "\n",
    "fps = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "for frame in tqdm(img_list):\n",
    "    img0 = cv2.imread(str(img_root/frame))[:, :, ::-1]\n",
    "    img0 = cv2.resize(img0, (img_rsz, img_rsz))\n",
    "\n",
    "    img_crops = []\n",
    "    pos_list = []\n",
    "    for row in range(0, img0.shape[1], steps):\n",
    "        for col in range(0, img0.shape[0], steps):\n",
    "            if img0[row:row+crop_sz, col:col+crop_sz].shape == (80, 80, 3):\n",
    "                img_crops.append(img0[row:row+crop_sz, col:col+crop_sz])\n",
    "                pos_list.append((row, col))\n",
    "                \n",
    "    model = model.eval()\n",
    "    pred_iou = []\n",
    "    with torch.no_grad():\n",
    "        for crop in img_crops:\n",
    "            img = crop.copy()\n",
    "            sample      = {'img': img}\n",
    "            sample      = transform(sample)\n",
    "            img         = sample['img']\n",
    "            img         = img.unsqueeze(0)\n",
    "            img = img.cuda()\n",
    "            out = model(img)[0]\n",
    "            pred_iou.append(out[0])\n",
    "            \n",
    "    pos = pos_list[np.argmax(pred_iou)]\n",
    "    \n",
    "    color_mask = np.zeros((img0.shape[0], img0.shape[1], 3))\n",
    "    color_mask[pos[0]:pos[0]+crop_sz, pos[1]:pos[1]+crop_sz, 1] = 1\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')\n",
    "    ax = fig.gca()\n",
    "    ax.imshow(img0)\n",
    "    ax.imshow(color_mask, alpha=0.5)\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    frames.append(data)\n",
    "    plt.close()\n",
    "    \n",
    "segment_clip = ImageSequenceClip(frames, fps=fps)\n",
    "name = str(Path(ROOT)/'mlcv-exp/data/saved'/'model_apt_{}.gif'.format(exp))\n",
    "segment_clip.write_gif(name, fps=fps)\n",
    "\n",
    "with open(name, 'rb') as f:\n",
    "    display(IPythonImage(data=f.read(), format='png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cfg = {\n",
    "    'img_root': Path(ROOT)/'EPIC_KITCHENS_2018'/'EK_frames',\n",
    "    'img_tmpl': 'img_{:05d}.jpg',\n",
    "    'img_rsz': 480,\n",
    "    'crop_sz': 80,\n",
    "    'len_data': 128,\n",
    "    'jitter': 0,\n",
    "    'batch_size': 32,\n",
    "    'num_workers': 8,\n",
    "    'show_pos_prob': 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, pred_frame_pts, img_path, max_epoch):\n",
    "    kwargs = {\n",
    "        'batch_size'    : test_cfg['batch_size'],\n",
    "        'shuffle'       : True,\n",
    "        'num_workers'   : test_cfg['num_workers'],\n",
    "        'sampler'       : None,\n",
    "        'pin_memory'    : True\n",
    "    }\n",
    "    \n",
    "    model = model.train()\n",
    "    mse = torch.nn.MSELoss()\n",
    "    \n",
    "    test_cfg['chosen_pt'] = pred_frame_pts\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(Train_Dataset(test_cfg, img_path), **kwargs)\n",
    "    for epoch in range(max_epoch):\n",
    "        for step, (img, iou) in enumerate(dataloader):\n",
    "#             fig, ax = plt.subplots()\n",
    "#             img = ImgToNumpy()(img)\n",
    "#             ax.imshow(img[0])\n",
    "#             plt.show()\n",
    "#             print(iou)\n",
    "\n",
    "            img = img.cuda()\n",
    "            iou = iou.type(torch.FloatTensor).cuda()\n",
    "\n",
    "            out = model(img)\n",
    "\n",
    "            loss = mse(out[:, 0], iou)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import ImageSequenceClip\n",
    "from tqdm import tqdm\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "from IPython.display import Image as IPythonImage\n",
    "\n",
    "fps = 12\n",
    "\n",
    "frames = []\n",
    "for frame in tqdm(img_list):\n",
    "    img0 = cv2.imread(str(img_root/frame))[:, :, ::-1]\n",
    "    img0 = cv2.resize(img0, (rsz, rsz))\n",
    "\n",
    "    img_crops = []\n",
    "    pos_list = []\n",
    "    for row in range(0, img0.shape[1], steps):\n",
    "        for col in range(0, img0.shape[0], steps):\n",
    "            if img0[row:row+crop_sz, col:col+crop_sz].shape == (80, 80, 3):\n",
    "                img_crops.append(img0[row:row+crop_sz, col:col+crop_sz])\n",
    "                pos_list.append((row, col))\n",
    "\n",
    "    model = model.eval()\n",
    "    pred_iou = []\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for crop in img_crops:\n",
    "            img = crop.copy()\n",
    "            sample      = {'img': img}\n",
    "            sample      = transform(sample)\n",
    "            img         = sample['img']\n",
    "            img         = img.unsqueeze(0)\n",
    "            img = img.cuda()\n",
    "            out = model(img)[0]\n",
    "            pred_iou.append(out[0])\n",
    "            \n",
    "    pos = pos_list[np.argmax(pred_iou)]\n",
    "    pos_cen = np.asarray(pos) + crop_sz//2\n",
    "    train_model(model, optimizer, pos_cen, str(img_root/frame), 1)\n",
    "    \n",
    "    color_mask = np.zeros((img0.shape[0], img0.shape[1], 3))\n",
    "    color_mask[pos[0]:pos[0]+crop_sz, pos[1]:pos[1]+crop_sz, 1] = 1\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')\n",
    "    ax = fig.gca()\n",
    "    ax.imshow(img0)\n",
    "    ax.imshow(color_mask, alpha=0.5)\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    frames.append(data)\n",
    "    plt.close()\n",
    "    \n",
    "segment_clip = ImageSequenceClip(frames, fps=fps)\n",
    "name = str(Path(ROOT)/'mlcv-exp/data/saved'/'model_apt_1.gif')\n",
    "segment_clip.write_gif(name, fps=fps)\n",
    "\n",
    "with open(name, 'rb') as f:\n",
    "    display(IPythonImage(data=f.read(), format='png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {'epoch': epoch, \n",
    "         'model_state_dict': model.state_dict(), \n",
    "         'optimizer_state_dict': optimizer.state_dict()}\n",
    "torch.save(state, Path(ROOT)/'mlcv-exp'/'data'/'weights'/'model_apt_5.state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
