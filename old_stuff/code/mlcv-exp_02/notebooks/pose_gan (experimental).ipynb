{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import cv2 \n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "\n",
    "WORK_DIR = Path(Path.cwd()).parent\n",
    "sys.path.append(str(WORK_DIR))\n",
    "from src import ROOT\n",
    "from src.datasets.transforms import *\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_id = 'Subject_1/open_juice_bottle/2/color'\n",
    "fps = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_set = Path(ROOT)/'mlcv-exp'/'data'/'labels'/'fpha_xyz_train.txt'\n",
    "xyz = np.loadtxt(split_set)\n",
    "\n",
    "split_set = Path(ROOT)/'mlcv-exp'/'data'/'labels'/'fpha_img_train.txt'\n",
    "with open(split_set, 'r') as f:\n",
    "    all_img_paths = f.read().splitlines()\n",
    "\n",
    "img_paths = []\n",
    "xyz_gt = []\n",
    "for i in range(len(all_img_paths)):\n",
    "    path = all_img_paths[i]\n",
    "    if seq_id in path:\n",
    "        img_paths.append(Path(ROOT)/img_root/path)\n",
    "        xyz_gt.append(xyz[i])\n",
    "uvd_gt = FPHA.xyz2uvd_color(np.reshape(xyz_gt, (-1, 21, 3)))\n",
    "uvd_gt[..., 0] *= 416/FPHA.ORI_WIDTH\n",
    "uvd_gt[..., 1] *= 416/FPHA.ORI_HEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import ImageSequenceClip\n",
    "from tqdm import tqdm\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "from IPython.display import Image as IPythonImage\n",
    "# seq = [x for x in sorted(seq_path.glob('*')) if x.is_file()]\n",
    "\n",
    "frames = []\n",
    "for f, u in tqdm(zip(img_paths, uvd_gt)):\n",
    "    img = get_img_dataloader(str(f), img_size)\n",
    "    img = img.unsqueeze(0).cuda()\n",
    "    img = ImgToNumpy()(img.cpu())[0]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = fig.gca()\n",
    "    ax.axis('off')\n",
    "    ax.imshow(img)\n",
    "    draw_joints(ax, u, 'r')\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    frames.append(data)\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "segment_clip = ImageSequenceClip(frames, fps=fps)\n",
    "name = str(Path(ROOT)/'mlcv-exp/data/saved'/'tmp.gif')\n",
    "segment_clip.write_gif(name, fps=fps)\n",
    "\n",
    "with open(name, 'rb') as f:\n",
    "    display(IPythonImage(data=f.read(), format='png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_set = Path(ROOT)/'mlcv-exp'/'data'/'labels'/'fpha_xyz_train.txt'\n",
    "xyz_gt = np.loadtxt(split_set)\n",
    "\n",
    "with open(Path(ROOT)/'mlcv-exp'/'data'/'labels'/'fpha_ar_seq_train.txt') as f:\n",
    "    img_list = f.read().splitlines()\n",
    "\n",
    "path_length = [int(i.split(' ')[1]) for i in img_list]\n",
    "action_cls  = [int(i.split(' ')[2]) for i in img_list]\n",
    "action_cls = [1 if i == 0 else 0 for i in action_cls]\n",
    "\n",
    "uvd_gt = FPHA.xyz2uvd_color(np.reshape(xyz_gt, (-1, 21, 3)))\n",
    "\n",
    "num_segments = 5\n",
    "track = 0\n",
    "seq_uvd_gt = []\n",
    "for num_frames in path_length:\n",
    "    seq_uvd_gt.append(uvd_gt[track:track+num_frames])\n",
    "    track += num_frames\n",
    "\n",
    "print(np.stack([seq_uvd_gt[0][1], seq_uvd_gt[0][100]]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Train_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        split_set = Path(ROOT)/'mlcv-exp'/'data'/'labels'/'fpha_xyz_train.txt'\n",
    "        xyz_gt = np.loadtxt(split_set)\n",
    "\n",
    "        with open(Path(ROOT)/'mlcv-exp'/'data'/'labels'/'fpha_ar_seq_train.txt') as f:\n",
    "            img_list = f.read().splitlines()\n",
    "\n",
    "        path_length = [int(i.split(' ')[1]) for i in img_list]\n",
    "        action_cls  = [int(i.split(' ')[2]) for i in img_list]\n",
    "        self.action_cls = [1 if i == 0 else 0 for i in action_cls]\n",
    "\n",
    "        uvd_gt = FPHA.xyz2uvd_color(np.reshape(xyz_gt, (-1, 21, 3)))\n",
    "        \n",
    "        self.num_segments = 5\n",
    "        track = 0\n",
    "        self.seq_uvd_gt = []\n",
    "        for num_frames in path_length:\n",
    "            self.seq_uvd_gt.append(uvd_gt[track:track+num_frames])\n",
    "            track += num_frames\n",
    "        \n",
    "        tfrm = []\n",
    "        tfrm.append(ImgToTorch())\n",
    "        self.transform = torchvision.transforms.Compose(tfrm)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        uvd = self.seq_uvd_gt[index]\n",
    "        action_id = self.action_cls[index]\n",
    "        \n",
    "        num_frames = len(uvd)\n",
    "        avg_duration = num_frames//self.num_segments\n",
    "        frames = list(np.multiply(list(range(num_segments)), avg_duration) + offset)\n",
    "        uvd_first = uvd[0].reshape(-1)\n",
    "        uvd_out = np.stack([uvd[i] for i in frames])\n",
    "        \n",
    "        sample      = {'img': uvd_out}\n",
    "        sample      = self.transform(sample)\n",
    "        crop        = sample['img']\n",
    "        return uvd_first, uvd_out, action_id\n",
    "    \n",
    "    def __len__(self):\n",
    "            return len(self.action_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'batch_size'    : 32,\n",
    "    'shuffle'       : True,\n",
    "    'num_workers'   : 8,\n",
    "    'sampler'       : None,\n",
    "    'pin_memory'    : True\n",
    "}\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(Train_Dataset(), **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(63, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(opt.channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = opt.img_size // 2 ** 4\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "\n",
    "        return validity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
