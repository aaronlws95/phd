{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from epic_kitchens.gulp.visualisation import FlowVisualiser\n",
    "from epic_kitchens.dataset.epic_dataset import EpicVideoDataset, EpicVideoFlowDataset, GulpVideoSegment\n",
    "from epic_kitchens.gulp.adapter import EpicDatasetAdapter\n",
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpha_action_file = '/4TB/aaron/First_Person_Action_Benchmark/action_object_info.txt'\n",
    "fpha_data = pd.read_csv(fpha_action_file, delim_whitespace=True)\n",
    "fpha_nouns = [x.replace('_', ' ') for x in fpha_data.object_name.unique()]\n",
    "\n",
    "fpha_verbs = fpha_data.action_name\n",
    "fpha_verbs = [x.split('_')[0] for x in fpha_verbs]\n",
    "fpha_verbs = list(dict.fromkeys(fpha_verbs))\n",
    "\n",
    "fpha_verb_noun = [x.split('_') for x in fpha_data.action_name]\n",
    "fpha_verb_noun = [' '.join(x) for x in fpha_verb_noun]\n",
    "print(fpha_verb_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_root = '/4TB/aaron/EPIC_KITCHENS_2018/'\n",
    "\n",
    "epic_action_data =  os.path.join(epic_root, 'annotations', 'EPIC_train_action_labels.csv')\n",
    "epic_action_data = pd.read_csv(epic_action_data)\n",
    "\n",
    "epic_nouns = [x.replace(':', ' ') for x in epic_action_data.noun.unique()]\n",
    "\n",
    "epic_verbs = epic_action_data.verb.unique()\n",
    "\n",
    "epic_verb_noun = [x + ' ' + y.replace(':', ' ') for x,y in zip(epic_action_data.verb, epic_action_data.noun)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('FPHA NOUNS:', fpha_nouns, '\\n')\n",
    "# print('EPIC_NOUNS:', epic_nouns, '\\n')\n",
    "print('Noun Intersection:', np.intersect1d(fpha_nouns, epic_nouns), \n",
    "      '\\'washing liquid (liquid soap)\\'', \n",
    "      '\\'can (soda can)\\'') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('FPHA VERBS:', fpha_verbs, '\\n')\n",
    "# print('EPIC_VERBS:', epic_verbs, '\\n')\n",
    "print('Verb Intersection:', np.intersect1d(fpha_verbs, epic_verbs))\n",
    "print('**Not the same kind of toast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('FPHA VERBS_NOUNS:', fpha_verb_noun, '\\n')\n",
    "# print('EPIC_VERBS_NOUNS:', epic_verb_noun, '\\n')\n",
    "fpha_and_epic_verb_nouns = list(np.intersect1d(fpha_verb_noun, epic_verb_noun))\n",
    "fpha_and_epic_verb_nouns.append('put teabag')\n",
    "fpha_and_epic_verb_nouns.append('close juice')\n",
    "fpha_and_epic_verb_nouns.append('open juice')\n",
    "fpha_and_epic_verb_nouns.append('pour juice')\n",
    "fpha_and_epic_verb_nouns.append('pour liquid washing')\n",
    "fpha_and_epic_verb_nouns.append('open can')\n",
    "fpha_and_epic_verb_nouns.append('pour wine')\n",
    "\n",
    "print('Verb + Noun Intersection:', fpha_and_epic_verb_nouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(epic_verb_noun, return_counts=True)\n",
    "epic_verb_noun_dict = dict(zip(unique, counts))\n",
    "for vn in fpha_and_epic_verb_nouns:\n",
    "    print(vn, epic_verb_noun_dict[vn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_object_data =  os.path.join(epic_root, 'annotations', 'EPIC_train_object_labels.csv')\n",
    "epic_object_data = pd.read_csv(epic_object_data)\n",
    "bb_list = epic_object_data.bounding_boxes.values.tolist()\n",
    "sum_bb = 0\n",
    "for i in bb_list:\n",
    "    if i != '[]':\n",
    "        sum_bb += 1\n",
    "        \n",
    "print(sum_bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_object_nouns = [x.replace(':', ' ') for x in epic_object_data.noun.unique()]\n",
    "epic_fpha_object_nouns = list(np.intersect1d(fpha_nouns, epic_object_nouns))\n",
    "epic_fpha_object_nouns.append('washing liquid')\n",
    "epic_fpha_object_nouns.append('cans')\n",
    "print('Object Intersection:', epic_fpha_object_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(epic_object_data.noun, return_counts=True)\n",
    "epic_noun_dict = dict(zip(unique, counts))\n",
    "for n in epic_fpha_object_nouns:\n",
    "    print(n, epic_noun_dict[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gulp_root = Path('/4TB/aaron/EPIC_KITCHENS_2018/epic_ar/data/processed/gulp')\n",
    "\n",
    "class_type = 'verb+noun'\n",
    "rgb_train = EpicVideoDataset(gulp_root / 'rgb_train', class_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, seg in enumerate(rgb_train.video_segments):\n",
    "#     if seg['verb'] == 'squeeze' and seg['noun'] == 'sponge':\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_seg = rgb_train.video_segments[749]\n",
    "epic_frames = rgb_train.load_frames(put_spoon_seg)\n",
    "epic_frames[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import ImageSequenceClip\n",
    "segment_clip = ImageSequenceClip([np.array(frame) for frame in epic_frames], fps=60)\n",
    "segment_clip.ipython_display()\n",
    "# segment_clip.write_videofile(\"squeeze_sponge_movie.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def get_data_list(modality, dataset_dir):\n",
    "    train_pairs = []\n",
    "    test_pairs = []\n",
    "    img_dir = os.path.join(dataset_dir, 'Video_files')\n",
    "    skel_dir = os.path.join(dataset_dir, 'Hand_pose_annotation_v1')\n",
    "    if modality == 'depth':\n",
    "        img_type = 'png'\n",
    "    else:\n",
    "        img_type = 'jpeg'\n",
    "    with open(os.path.join(dataset_dir, 'data_split_action_recognition.txt')) as f:\n",
    "        cur_split = 'Training'\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            words = l.split()\n",
    "            if(words[0] == 'Training' or words[0] == 'Test'):\n",
    "                cur_split = words[0]\n",
    "            else:\n",
    "                path = l.split()[0]\n",
    "                full_path = os.path.join(img_dir, path, modality)\n",
    "                len_frame_idx = len([x for x in os.listdir(full_path)\n",
    "                                    if os.path.join(full_path, x)])\n",
    "                skeleton_path = os.path.join(skel_dir, path, 'skeleton.txt')\n",
    "                skeleton_vals = np.loadtxt(skeleton_path)\n",
    "                for i in range(len_frame_idx):\n",
    "                    img_path = os.path.join(img_dir, path, modality, '%s_%04d.%s' %(modality, i, img_type))\n",
    "                    skel_xyz = skeleton_vals[:, 1:].reshape(skeleton_vals.shape[0], -1)[i]\n",
    "                    data_pair = (img_path, skel_xyz)\n",
    "                    if cur_split == 'Training':\n",
    "                        train_pairs.append(data_pair)\n",
    "                    else:\n",
    "                        test_pairs.append(data_pair)\n",
    "    return train_pairs, test_pairs\n",
    "\n",
    "root = '/4TB/aaron/First_Person_Action_Benchmark'\n",
    "train_pairs, test_pairs = get_data_list('color', root)\n",
    "file_name = [i for i,j in train_pairs]\n",
    "train_file_name = [file.split('/') for file in file_name]\n",
    "file_name = [i for i,j in test_pairs]\n",
    "test_file_name = [file.split('/') for file in file_name]\n",
    "\n",
    "fpha_frames = []\n",
    "file_name = test_file_name\n",
    "for idx in range(3705, 3778):\n",
    "\n",
    "    subject = file_name[idx][5]\n",
    "    action_name = file_name[idx][6]\n",
    "    seq_idx = file_name[idx][7]\n",
    "    frame_idx = int(file_name[idx][-1][-9:-5])\n",
    "\n",
    "    sample = {\n",
    "        'subject': subject,\n",
    "        'action_name': action_name,\n",
    "        'seq_idx': seq_idx,\n",
    "        'frame_idx': frame_idx,\n",
    "    }\n",
    "    \n",
    "    img_path = os.path.join(root, 'Video_files', sample['subject'],\n",
    "                            sample['action_name'], sample['seq_idx'], 'color',\n",
    "                            'color_{:04d}.jpeg'.format(sample['frame_idx']))\n",
    "    fpha_frames.append(np.asarray(Image.open(img_path)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from moviepy.editor import ImageSequenceClip\n",
    "# segment_clip = ImageSequenceClip([np.array(frame) for frame in fpha_frames], fps=60)\n",
    "# segment_clip.ipython_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
