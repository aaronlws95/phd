{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "WORK_DIR = Path(Path.cwd()).parent\n",
    "sys.path.append(str(WORK_DIR))\n",
    "from src.datasets import get_dataset, get_dataloader\n",
    "from src.utils import parse_data_cfg, IMG, FPHA, LMDB, DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_fold = 'exp2'\n",
    "cfgname = 'fpha_hpo_2hand_1_data_rand_alt'\n",
    "cfg = parse_data_cfg(WORK_DIR/'data_cfg'/exp_fold/(cfgname + '.cfg'))\n",
    "epoch = 200\n",
    "exp_dir = cfg[\"exp_dir\"]\n",
    "data_split = 'test'\n",
    "split_set = cfg[data_split + '_set']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Flip entire dataset batch to same side\n",
    "    Args:\n",
    "        batch   : list of img, bbox_gt, uvd_gt\n",
    "        img     : [img_1, ..., img_batch]\n",
    "        bbox_gt : [bbox_gt_1, ..., bbox_gt_batch]\n",
    "        uvd_gt  : [uvd_gt_1, ..., uvd_gt_batch]\n",
    "    Out:\n",
    "        Vertically mirrored inputs\n",
    "    \"\"\"\n",
    "    FT          = torch.FloatTensor\n",
    "    img, uvd_gt = zip(*batch)\n",
    "    flip        = random.randint(1, 10000)%2\n",
    "    # Do flipping\n",
    "    # 0 = left, 1 = right\n",
    "    hand_side = 1\n",
    "    if flip:\n",
    "        hand_side = 0  \n",
    "\n",
    "    new_img     = []\n",
    "    new_uvd     = []\n",
    "    for i, u in batch:\n",
    "        if flip:\n",
    "            i       = i.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            u[:, 0] = 0.999 - u[:, 0]\n",
    "        i = np.asarray(i)\n",
    "        i = i/255.0\n",
    "        i = IMG.imgshape2torch(i)\n",
    "        new_img.append(i)\n",
    "        new_uvd.append(u)\n",
    "\n",
    "    new_img     = FT(new_img)\n",
    "    new_uvd     = FT(new_uvd)\n",
    "    return new_img, new_uvd, hand_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_kwargs = {'split_set': split_set}\n",
    "dataset   = get_dataset(cfg, dataset_kwargs)\n",
    "sampler   = None\n",
    "shuffle   = cfg['shuffle']\n",
    "kwargs = {'batch_size'  : int(cfg['batch_size']),\n",
    "          'shuffle'     : shuffle,\n",
    "          'num_workers' : int(cfg['num_workers']),\n",
    "          'pin_memory'  : True,\n",
    "          'collate_fn'  : collate_fn}\n",
    "data_loader = get_dataloader(dataset, sampler, kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0 \n",
    "for i, (img, uvd_gt, hand_side) in enumerate(data_loader):\n",
    "    if i == idx:\n",
    "        if hand_side == 0:\n",
    "            print('left')\n",
    "        else:\n",
    "            print('right')        \n",
    "        batch_size = img.shape[0]\n",
    "        img = img.cpu().numpy()\n",
    "        img = np.swapaxes(img, 2, 3)\n",
    "        img = np.swapaxes(img, 1, 3)\n",
    "        img = IMG.scale_img_255(img)\n",
    "        uvd_gt = uvd_gt.cpu().numpy()\n",
    "        uvd_gt = np.squeeze(uvd_gt)\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 4, figsize=(15, 15))\n",
    "idx = 0\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        k = np.ravel_multi_index((i, j), (4, 4))\n",
    "        if k >= len(img):\n",
    "            break        \n",
    "        cur_img = img[idx]\n",
    "        u = uvd_gt[idx]        \n",
    "        ax[i, j].imshow(cur_img)\n",
    "        u = IMG.scale_points_WH(u, (1,1), (cur_img.shape[0], cur_img.shape[1]))\n",
    "        FPHA.visualize_joints_2d(ax[i, j], u[FPHA.REORDER_IDX], joint_idxs=False)\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = LMDB.get_keys(os.path.join(DATA_DIR, split_set + \"_keys_cache.p\"))\n",
    "xyz_gt = LMDB.read_all_lmdb_dataroot(keys, os.path.join(DATA_DIR, split_set + '_xyz_gt.lmdb'), 'float32', (21, 3))\n",
    "uvd_gt = FPHA.xyz2uvd_color(xyz_gt)\n",
    "\n",
    "# Left hand\n",
    "pred_file = os.path.join(DATA_DIR, exp_dir, 'predict_{}_{}_uvd_left.txt'.format(epoch, data_split))\n",
    "pred_uvd_left = np.reshape(np.loadtxt(pred_file), (-1, 21, 3))\n",
    "pred_uvd_left = IMG.scale_points_WH(pred_uvd_left, (1, 1), (1920, 1080))\n",
    "pred_uvd_left[..., 2] *= 1000\n",
    "pred_xyz_left = FPHA.uvd2xyz_color(pred_uvd_left)\n",
    "pred_file = os.path.join(DATA_DIR, exp_dir, 'predict_{}_{}_conf_left.txt'.format(epoch, data_split))\n",
    "pred_conf_left = np.loadtxt(pred_file)\n",
    "\n",
    "# Right hand\n",
    "pred_file = os.path.join(DATA_DIR, exp_dir, 'predict_{}_{}_uvd_right.txt'.format(epoch, data_split))\n",
    "pred_uvd_right = np.reshape(np.loadtxt(pred_file), (-1, 21, 3))\n",
    "pred_uvd_right = IMG.scale_points_WH(pred_uvd_right, (1, 1), (1920, 1080))\n",
    "pred_uvd_right[..., 2] *= 1000\n",
    "pred_xyz_right = FPHA.uvd2xyz_color(pred_uvd_right)\n",
    "pred_file = os.path.join(DATA_DIR, exp_dir, 'predict_{}_{}_conf_right.txt'.format(epoch, data_split))\n",
    "pred_conf_right = np.loadtxt(pred_file)\n",
    "\n",
    "if cfg['pred_img_side'] == 'left':\n",
    "    uvd_gt = IMG.scale_points_WH(uvd_gt, (1920, 1080), (1,1))\n",
    "    uvd_gt[: , :, 0] = 0.999 - uvd_gt[:, :, 0]\n",
    "    uvd_gt = IMG.scale_points_WH(uvd_gt, (1,1), (1920, 1080))\n",
    "    xyz_gt = FPHA.uvd2xyz_color(uvd_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 15427\n",
    "# idx = 21664\n",
    "idx = 42863\n",
    "print(keys[idx])\n",
    "\n",
    "img = Image.open(os.path.join(DATA_DIR, 'First_Person_Action_Benchmark', 'Video_files', keys[idx]))\n",
    "\n",
    "if cfg['pred_img_side'] == 'left':\n",
    "    img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "img = np.asarray(img)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(img)\n",
    "\n",
    "FPHA.visualize_joints_2d(ax, pred_uvd_left[idx][FPHA.REORDER_IDX], joint_idxs=False, c='r')\n",
    "FPHA.visualize_joints_2d(ax, pred_uvd_right[idx][FPHA.REORDER_IDX], joint_idxs=False, c='g')\n",
    "FPHA.visualize_joints_2d(ax, uvd_gt[idx][FPHA.REORDER_IDX], joint_idxs=False, c='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side = 'left'\n",
    "if side == 'right':\n",
    "    pred_uvd = pred_uvd_right\n",
    "    pred_xyz = pred_xyz_right\n",
    "    pred_conf = pred_conf_right\n",
    "else:\n",
    "    pred_uvd = pred_uvd_left\n",
    "    pred_xyz = pred_xyz_left\n",
    "    pred_conf = pred_conf_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_idx = []\n",
    "# get the best idx for each 2D cell\n",
    "for i in range(len(pred_conf[idx])//5):\n",
    "    max_idx.append(i*5 + np.argmax(pred_conf[idx][i*5:i*5+5]))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "pred_uvd_416 = IMG.scale_points_WH(pred_uvd[idx], (1920, 1080), (416, 416))\n",
    "uvd_gt_416 = IMG.scale_points_WH(uvd_gt[idx], (1920, 1080), (416, 416))\n",
    "FPHA.visualize_joints_2d(ax, pred_uvd_416[FPHA.REORDER_IDX], joint_idxs=False, c='r')\n",
    "img_rsz = IMG.resize_img(img, (416, 416))\n",
    "ax.imshow(img_rsz.astype('uint32'))\n",
    "\n",
    "# red is the best\n",
    "# yellow is anything over 0.9\n",
    "import matplotlib.patches as patches\n",
    "for i in range(len(max_idx)):\n",
    "    index = np.unravel_index(i, (13, 13))\n",
    "    x = index[1]\n",
    "    y = index[0]\n",
    "    al = pred_conf[idx][max_idx[i]]\n",
    "    if al == np.amax(pred_conf[idx]):\n",
    "        c = 'r'\n",
    "    elif al <= 0.8:\n",
    "        c = 'b'\n",
    "    else:\n",
    "        c = 'y'\n",
    "    rect = patches.Rectangle((x*32,y*32),32,32,linewidth=1, edgecolor=c, facecolor=c, fill=True, alpha=al)\n",
    "    ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%s UVD mean_l2_error: ' %data_split, FPHA.mean_L2_error(uvd_gt[:len(pred_uvd)], pred_uvd))\n",
    "print('%s XYZ mean_l2_error: ' %data_split, FPHA.mean_L2_error(xyz_gt[:len(pred_uvd)], pred_xyz))\n",
    "error = []\n",
    "for i, (pred, uvd) in enumerate(zip(pred_uvd, uvd_gt)):\n",
    "#     print(i, FPHA.mean_L2_error(uvd, pred))\n",
    "    error.append(FPHA.mean_L2_error(uvd, pred))\n",
    "error = np.asarray(error)\n",
    "min_error_idx = np.argmin(error)\n",
    "max_error_idx = np.argmax(error)\n",
    "print('Best Pose id:', min_error_idx, 'uvd_error:', error[min_error_idx])\n",
    "print('Worst Pose id:', max_error_idx, 'uvd_error:', error[max_error_idx])\n",
    "for idx in np.argsort(error):\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pck = FPHA.percentage_frames_within_error_curve(xyz_gt[:len(pred_uvd)], pred_xyz)\n",
    "pck_str = ''\n",
    "for p in pck:\n",
    "    pck_str += str(p) + ', '\n",
    "print(pck_str)\n",
    "thresholds = np.arange(0, 85, 5)\n",
    "print('AUC:', FPHA.calc_auc(pck, thresholds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import ImageSequenceClip\n",
    "from tqdm import tqdm\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "\n",
    "seq = 'Subject_3/put_salt/3'\n",
    "# seq = 'Subject_3/squeeze_paper/3'\n",
    "# seq = 'Subject_2/charge_cell_phone/1'\n",
    "SAVE_DIR = Path(DATA_DIR)/'acv-data'/'gifs'\n",
    "seq_keys_list = [(i, k.split('/')) for i, k in enumerate(keys) if seq in k]\n",
    "\n",
    "index_list, seq_list = zip(*seq_keys_list)\n",
    "\n",
    "seq_list = [int(i[-1].split('_')[-1].split('.')[0]) for i in seq_list]\n",
    "ind = np.argsort(seq_list).astype('uint32')\n",
    "\n",
    "index_list = np.asarray(index_list)\n",
    "index_list = index_list[ind]\n",
    "\n",
    "frames = []\n",
    "for idx in tqdm(index_list):\n",
    "    idx = int(idx)\n",
    "    img = Image.open(os.path.join(DATA_DIR, 'First_Person_Action_Benchmark', 'Video_files', keys[idx]))\n",
    "    if cfg['pred_img_side'] == 'left':\n",
    "        img = img.transpose(Image.FLIP_LEFT_RIGHT)    \n",
    "    img = np.asarray(img)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = fig.gca()\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    FPHA.visualize_joints_2d(ax, pred_uvd_left[idx][FPHA.REORDER_IDX], joint_idxs=False, c='r')\n",
    "    FPHA.visualize_joints_2d(ax, pred_uvd_right[idx][FPHA.REORDER_IDX], joint_idxs=False, c='g')\n",
    "    FPHA.visualize_joints_2d(ax, uvd_gt[idx][FPHA.REORDER_IDX], joint_idxs=False, c='b')\n",
    "    fig.canvas.draw()\n",
    "    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    frames.append(data)\n",
    "\n",
    "    plt.close() \n",
    "    \n",
    "segment_clip = ImageSequenceClip(frames, fps=24)\n",
    "name = SAVE_DIR/('{}_{}_{}_{}.gif'.format(cfgname, epoch, data_split, seq.replace('/', '_')))\n",
    "segment_clip.write_gif(name, fps=24)\n",
    "from IPython.display import Image as IPythonImage\n",
    "with open(name,'rb') as f:\n",
    "    display(IPythonImage(data=f.read(), format='png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from src.models import get_model\n",
    "from src.utils import EK\n",
    "\n",
    "# cfg['device'] = 2\n",
    "model = get_model(cfg, False, epoch, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image\n",
    "\n",
    "img = Image.open(EK.get_img_path(22, 16, 11616))\n",
    "# idx = 1000\n",
    "# img = Image.open(os.path.join(DATA_DIR, 'First_Person_Action_Benchmark', 'Video_files', keys[idx]))\n",
    "pred = model.detect(img)\n",
    "pred_left, pred_right = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img)\n",
    "FPHA.visualize_joints_2d(ax, pred_left[FPHA.REORDER_IDX], joint_idxs=False, c='r')\n",
    "FPHA.visualize_joints_2d(ax, pred_right[FPHA.REORDER_IDX], joint_idxs=False, c='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video\n",
    "dom = 'action'\n",
    "modality = 'rgb'\n",
    "data_split_vid = 'train'\n",
    "\n",
    "# Single vid\n",
    "vid_idx = 7119\n",
    "all_img_path = EK.get_video_frames(vid_idx, dom=dom, modality=modality, data_split=data_split_vid)\n",
    "\n",
    "# Multi vid\n",
    "# start_vid = 54\n",
    "# end_vid = 57\n",
    "# vid_idx = '{}-{}'.format(start_vid, end_vid)\n",
    "# all_img_path = []\n",
    "# for idx in range(54, end_vid + 1):\n",
    "#     cur_img_paths = EK.get_video_frames(idx, dom=dom, modality=modality, data_split=data_split_vid)\n",
    "#     all_img_path += cur_img_paths\n",
    "\n",
    "# print(len(all_img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import ImageSequenceClip\n",
    "from tqdm import tqdm\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "import time\n",
    "SAVE_DIR = Path(DATA_DIR)/'acv-data'/'gifs'\n",
    "\n",
    "frames = []\n",
    "total_time = 0\n",
    "for img_path in tqdm(all_img_path):\n",
    "    img = Image.open(img_path)\n",
    "    start = time.time()\n",
    "    pred_left, pred_right = model.detect(img)\n",
    "    end = time.time()\n",
    "    total_time += (end - start)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = fig.gca()\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    FPHA.visualize_joints_2d(ax, pred_left[FPHA.REORDER_IDX], joint_idxs=False, c='r')\n",
    "    FPHA.visualize_joints_2d(ax, pred_right[FPHA.REORDER_IDX], joint_idxs=False, c='g')\n",
    "    fig.canvas.draw()\n",
    "    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    frames.append(data)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "print('fps:', len(all_img_path)/total_time)\n",
    "segment_clip = ImageSequenceClip(frames, fps=60)\n",
    "name = SAVE_DIR/('{}_{}_{}_{}_{}_{}.gif'.format(cfgname, epoch, vid_idx, dom, modality, data_split_vid))\n",
    "segment_clip.write_gif(name, fps=6)\n",
    "from IPython.display import Image as IPythonImage\n",
    "with open(name,'rb') as f:\n",
    "    display(IPythonImage(data=f.read(), format='png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
